import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers,
                 dropout=0,
                 bidirectional=False):
        """
        Args:
            input_size (int): The expected number of features in the input.
            hidden_size (int): The number of features in the hidden state.
            num_layers (int): Number of recurrent layers.
            dropout (float): dropout probability.
            bidirectional (boolean): whether to use bidirectional model.
        """
        super(Encoder, self).__init__()
        self.gru = nn.GRU(input_size,
                          hidden_size,
                          num_layers,
                          dropout=dropout if num_layers else 0,
                          bidirectional=bidirectional)

    def forward(self, input, hidden):
        """
        Args:
            input (seq_len, batch, input_size): Input sequence.
            hidden (num_layers*num_directions, batch, hidden_size): \
                    Initial states of the GRU.

        Returns:
            output (seq_len, batch, num_directions*hidden_size): Outputs of GRU at every step.
            hidden (num_layers*num_directions, batch, hidden_size): Updated states of the GRU.
        """
        # Feed source sequences into GRU:
        output, hidden = self.gru(input, hidden)
        return output, hidden

class Decoder(nn.Module):
    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers,
                 dropout=0,
                 output_size):
        """
        Args:
            input_size (int): The expected number of features in the input.
            hidden_size (int): The number of features in the hidden state.
            num_layers (int): Number of recurrent layers.
            dropout (float): dropout probability.
        """
        super(Decoder, self).__init__()
        self.gru = nn.GRU(input_size,
                          hidden_size,
                          num_layers,
                          dropout=dropout if num_layers else 0)
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, hidden, target=None, teacher_forcing=False):
        """
        Args:
            hidden (num_layers, batch, hidden_size): States of the GRU.
            target (seq_len, batch, input_size): Target sequence. If None,
                the output sequence is generated by feeding the output
                of the previous timestep (teacher_forcing has to be False).
            teacher_forcing (bool): Whether to use teacher forcing or not.

        Returns:
            outputs (max_out_seq_length, batch, output_size): Tensor of log-probabilities
                of words in the target language.
            hidden of shape (1, batch_size, hidden_size): New states of the GRU.

        Note: Do not forget to transfer tensors that you may want to create in this function to the device
        specified by `hidden.device`.
        """
        if target is None:
            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'

        # Determine length of the sequence:
        seq_len = MAX_LENGTH if target is None else target.shape[0]
        # Sequence to record the predicted words:
        seqs = torch.zeros((seq_len, hidden.shape[1], self.tgt_dictionary_size), device=hidden.device)
        # The starting word to feed to the GRU:
        word = torch.tensor([SOS_token], device=hidden.device).repeat((1, hidden.shape[1]))
        if pad_tgt_seqs is not None:
            pad_tgt_seqs = torch.cat([word, pad_tgt_seqs], dim=0)
        for i in range(seq_len):
            # Embed the input (output):
            word = self.embedding(pad_tgt_seqs[None,i,:]) if teacher_forcing else self.embedding(word)
            # Feed the output and the hidden to the GRU:
            output, hidden = self.gru(F.relu(word), hidden)
            # Compute the log-softmax of output from GRU to get word in target language:
            output = F.log_softmax(self.out(output), dim=2)
            # Record the generated words:
            seqs[i] = output
            # Get the index presentation of the output:
            word = torch.argmax(output, dim=2)
        return seqs, hidden


